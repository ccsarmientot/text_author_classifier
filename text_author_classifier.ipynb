{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Libraries and utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ccsar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'say love place'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove accents\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8')\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "\n",
    "    # Remove English stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    text = ' '.join(tokens)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Limpiamos el texto\n",
    "preprocess_text('What can I say, I love this place')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of dataframe: (500, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49339</th>\n",
       "      <td>into her simultaneously from the the and the u...</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45580</th>\n",
       "      <td>his heels holding a pan over a camp fire he lo...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51146</th>\n",
       "      <td>which has the atlantic still it has exhibited ...</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37861</th>\n",
       "      <td>l of the region to something more from them in...</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>the train was starting wrung his companion s h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  author\n",
       "49339  into her simultaneously from the the and the u...      45\n",
       "45580  his heels holding a pan over a camp fire he lo...      42\n",
       "51146  which has the atlantic still it has exhibited ...      48\n",
       "37861  l of the region to something more from them in...      36\n",
       "193    the train was starting wrung his companion s h...       1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_path = r'Data\\Gungor_2018_VictorianAuthorAttribution_data-train.csv'\n",
    "# df = pd.read_csv(data_path, encoding='latin-1')\n",
    "\n",
    "## URL from github repo, load as dataframe\n",
    "url = 'https://raw.githubusercontent.com/ccsarmientot/text_author_classifier/master/datasets/sample_victorian.parquet'\n",
    "url = 'datasets/sample_victorian.parquet'\n",
    "df = pd.read_parquet(url)\n",
    "\n",
    "print(f'Shape of dataframe: {df.shape}')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cantidad promedio de caracteres por texto: 4,963.65'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_chr = np.mean(df['text'].apply(len))\n",
    "f'Cantidad promedio de caracteres por texto: {avg_chr:,.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cantidad promedio de palabras por texto: 1,001.00'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_chr = np.mean(df['text'].apply(lambda x: len(x.split(' '))))\n",
    "f'Cantidad promedio de palabras por texto: {avg_chr:,.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author\n",
       "8     73\n",
       "26    37\n",
       "14    27\n",
       "37    25\n",
       "39    24\n",
       "45    24\n",
       "21    20\n",
       "33    20\n",
       "9     14\n",
       "41    14\n",
       "19    13\n",
       "15    13\n",
       "32    12\n",
       "38    12\n",
       "48    12\n",
       "25    11\n",
       "43    10\n",
       "30     9\n",
       "4      9\n",
       "46     9\n",
       "10     9\n",
       "35     8\n",
       "50     8\n",
       "44     8\n",
       "1      8\n",
       "20     7\n",
       "18     7\n",
       "17     6\n",
       "36     6\n",
       "29     5\n",
       "42     5\n",
       "28     5\n",
       "24     4\n",
       "12     4\n",
       "3      3\n",
       "2      3\n",
       "22     3\n",
       "11     2\n",
       "27     2\n",
       "23     2\n",
       "13     2\n",
       "34     2\n",
       "6      2\n",
       "40     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Se identifica un desbalance de clases:\n",
    "df['author'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Getting sample fo \n",
    "# df_sample = df.sample(10_000)\n",
    "# n_authors = df_sample['author'].nunique()\n",
    "# print(f'Authors in df_sample: {n_authors}')\n",
    "# df_sample.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos librerias\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "results = []\n",
    "X = df['text']\n",
    "y = df['author']\n",
    "\n",
    "# Dividimos los datos en entrenamiento y testeo\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se aplica el preprocesameinto\n",
    "\n",
    "# 1. Feature Extraction\n",
    "tfidf = TfidfVectorizer(max_features=1000, preprocessor=preprocess_text)\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "# 2. Oversampling\n",
    "oversample = RandomOverSampler()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Define function to iterate over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv_models(model_name:str, classifier, \n",
    "                    param_grid:dict[list], \n",
    "                    X_train, y_train) -> dict:\n",
    "\n",
    "    # Creacion del pipeline del modelo inicial\n",
    "    model = Pipeline(steps=[\n",
    "        ## Se aplica el modelo\n",
    "        (model_name, classifier)\n",
    "    ])\n",
    "\n",
    "    ######################## PRIMERA BÚSQUEDA DE PARÁMETROS ####################\n",
    "    print(' Primera búsqueda de parámetros '.center(80, '#'))\n",
    "\n",
    "    # Creamos el objeto GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=2, verbose=2)\n",
    "\n",
    "    # Ajustamos el modelo a los datos de entrenamiento\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Obtenemos los mejores parámetros\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f'Resultados primera búsqueda: {best_params}', '\\n')\n",
    "\n",
    "    ######################## SEGUNDA BÚSQUEDA DE PARÁMETROS ####################\n",
    "    print(' Segunda búsqueda de parámetros '.center(80, '#'), '\\n')\n",
    "    \n",
    "\n",
    "    # A los parámetros que deben ser enteros se resta y suma 1\n",
    "    int_list = ['min_samples_split', 'max_depth', 'n_neighbors', 'min_samples_leaf']\n",
    "    int_grid = {k:[v-1, v, v+1] for k,v in best_params.items() if any([i in k for i in int_list])}\n",
    "    ## Clean zero values\n",
    "    int_grid = {k:[c for c in v if c != 0] for k,v in int_grid.items()}\n",
    "\n",
    "    # A los parámetros numéricos encontrados se resta y suma el 10 %\n",
    "    int_params_grid = {k:[v-(v/10), v, v+(v/10)] for k,v in best_params.items() if isinstance(v, (float, int))}\n",
    "    \n",
    "    # A los parámetros en formato string encontrados se deja el mejor\n",
    "    str_params_grid = {k:[v] for k,v in best_params.items() if not isinstance(v, list)}\n",
    "\n",
    "    best_params_grid = {**str_params_grid, **int_params_grid, **int_grid}\n",
    "    # best_params_grid['tfidf__max_features'] = [1000]\n",
    "\n",
    "    print('DEBUG: params after transform: ', best_params_grid)\n",
    "\n",
    "    # Creamos el objeto GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=best_params_grid, cv=2, verbose=2)\n",
    "\n",
    "    # Ajustamos el modelo a los datos de entrenamiento\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Obtenemos los mejores parámetros\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f'Resultados segunda búsqueda: {best_params}', '\\n')\n",
    "\n",
    "    return best_params\n",
    "\n",
    "\n",
    "def train_final_model(model_name, classifier, X_train, y_train) -> list:\n",
    "\n",
    "    print(' Creando modelo final con los mejores parámetros '.center(80, '#'))\n",
    "    \n",
    "    # Creacion del pipeline modelo final\n",
    "    final_model = Pipeline(steps=[\n",
    "        ## Se aplica el modelo\n",
    "        (model_name, classifier)\n",
    "    ])\n",
    "\n",
    "    # Ajuste del modelo\n",
    "    final_model.fit(X_train, y_train)\n",
    "\n",
    "    # Medimos el accuracy del modelo\n",
    "    accuracy = final_model.score(X_test, y_test)\n",
    "    print(f'Accuracy de {model_name}: {accuracy:,.2%}')\n",
    "\n",
    "    # Se predicen las clases para test\n",
    "    y_pred = final_model.predict(X_test)\n",
    "\n",
    "    p, r, f1, s = precision_recall_fscore_support(y_test, y_pred, average='weighted')\n",
    "    return [model_name, accuracy, p, r, f1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Primera búsqueda de parámetros ########################\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ..........................LogisticRegression__C=0.1; total time=   0.7s\n",
      "[CV] END ..........................LogisticRegression__C=0.1; total time=   0.3s\n",
      "[CV] END ............................LogisticRegression__C=1; total time=   3.6s\n",
      "[CV] END ............................LogisticRegression__C=1; total time=   2.0s\n",
      "[CV] END ...........................LogisticRegression__C=10; total time=   3.2s\n",
      "[CV] END ...........................LogisticRegression__C=10; total time=   3.2s\n",
      "Resultados primera búsqueda: {'LogisticRegression__C': 10} \n",
      "\n",
      "######################## Segunda búsqueda de parámetros ######################## \n",
      "\n",
      "DEBUG: params after transform:  {'LogisticRegression__C': [9.0, 10, 11.0]}\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ..........................LogisticRegression__C=9.0; total time=   1.1s\n",
      "[CV] END ..........................LogisticRegression__C=9.0; total time=   4.4s\n",
      "[CV] END ...........................LogisticRegression__C=10; total time=   4.5s\n",
      "[CV] END ...........................LogisticRegression__C=10; total time=   1.5s\n",
      "[CV] END .........................LogisticRegression__C=11.0; total time=   2.5s\n",
      "[CV] END .........................LogisticRegression__C=11.0; total time=   1.2s\n",
      "Resultados segunda búsqueda: {'LogisticRegression__C': 9.0} \n",
      "\n",
      "############### Creando modelo final con los mejores parámetros ################\n",
      "Accuracy de LogisticRegression: 47.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Definimos el modelo a usar\n",
    "classifier = LogisticRegression()\n",
    "model_name = 'LogisticRegression'\n",
    "\n",
    "# Definimos los parámetros a explorar\n",
    "param_grid = {\n",
    "    \n",
    "    f'{model_name}__C': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "best_params = train_cv_models(model_name, classifier, param_grid, X_train, y_train)\n",
    "model_best_params = {k.split('__')[1]:v for k,v in best_params.items() if 'tfidf' not in k}\n",
    "\n",
    "best_classifier = LogisticRegression(**model_best_params)\n",
    "model_stats = train_final_model(model_name, best_classifier, X_train, y_train)\n",
    "results.append(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Primera búsqueda de parámetros ########################\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ..............................NaiveBayes__alpha=0.1; total time=   0.0s\n",
      "[CV] END ..............................NaiveBayes__alpha=0.1; total time=   0.0s\n",
      "[CV] END ................................NaiveBayes__alpha=1; total time=   0.0s\n",
      "[CV] END ................................NaiveBayes__alpha=1; total time=   0.0s\n",
      "[CV] END ...............................NaiveBayes__alpha=10; total time=   0.0s\n",
      "[CV] END ...............................NaiveBayes__alpha=10; total time=   0.0s\n",
      "Resultados primera búsqueda: {'NaiveBayes__alpha': 0.1} \n",
      "\n",
      "######################## Segunda búsqueda de parámetros ######################## \n",
      "\n",
      "DEBUG: params after transform:  {'NaiveBayes__alpha': [0.09000000000000001, 0.1, 0.11]}\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END ..............NaiveBayes__alpha=0.09000000000000001; total time=   0.0s\n",
      "[CV] END ..............NaiveBayes__alpha=0.09000000000000001; total time=   0.0s\n",
      "[CV] END ..............................NaiveBayes__alpha=0.1; total time=   0.0s\n",
      "[CV] END ..............................NaiveBayes__alpha=0.1; total time=   0.0s\n",
      "[CV] END .............................NaiveBayes__alpha=0.11; total time=   0.0s\n",
      "[CV] END .............................NaiveBayes__alpha=0.11; total time=   0.0s\n",
      "Resultados segunda búsqueda: {'NaiveBayes__alpha': 0.09000000000000001} \n",
      "\n",
      "############### Creando modelo final con los mejores parámetros ################\n",
      "Accuracy de NaiveBayes: 38.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Definimos el modelo a usar\n",
    "classifier = MultinomialNB()\n",
    "model_name = 'NaiveBayes'\n",
    "\n",
    "# Definimos los parámetros a explorar\n",
    "param_grid = {\n",
    "    \n",
    "    f'{model_name}__alpha': [0.1, 1, 10],\n",
    "}\n",
    "\n",
    "\n",
    "best_params = train_cv_models(model_name, classifier, param_grid, X_train, y_train)\n",
    "model_best_params = {k.split('__')[1]:v for k,v in best_params.items() if 'tfidf' not in k}\n",
    "\n",
    "best_classifier = MultinomialNB(**model_best_params)\n",
    "model_stats = train_final_model(model_name, best_classifier, X_train, y_train)\n",
    "results.append(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 KNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Primera búsqueda de parámetros ########################\n",
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=3; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=3; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=5; total time=   0.6s\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=5; total time=   0.6s\n",
      "[CV] END KNeighbors__metric=manhattan, KNeighbors__n_neighbors=3; total time=   1.1s\n",
      "[CV] END KNeighbors__metric=manhattan, KNeighbors__n_neighbors=3; total time=   0.7s\n",
      "[CV] END KNeighbors__metric=manhattan, KNeighbors__n_neighbors=5; total time=   0.6s\n",
      "[CV] END KNeighbors__metric=manhattan, KNeighbors__n_neighbors=5; total time=   0.6s\n",
      "[CV] END KNeighbors__metric=cosine, KNeighbors__n_neighbors=3; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=cosine, KNeighbors__n_neighbors=3; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=cosine, KNeighbors__n_neighbors=5; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=cosine, KNeighbors__n_neighbors=5; total time=   0.5s\n",
      "Resultados primera búsqueda: {'KNeighbors__metric': 'euclidean', 'KNeighbors__n_neighbors': 3} \n",
      "\n",
      "######################## Segunda búsqueda de parámetros ######################## \n",
      "\n",
      "DEBUG: params after transform:  {'KNeighbors__metric': ['euclidean'], 'KNeighbors__n_neighbors': [2, 3, 4]}\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=2; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=2; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=3; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=3; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=4; total time=   0.5s\n",
      "[CV] END KNeighbors__metric=euclidean, KNeighbors__n_neighbors=4; total time=   0.5s\n",
      "Resultados segunda búsqueda: {'KNeighbors__metric': 'euclidean', 'KNeighbors__n_neighbors': 2} \n",
      "\n",
      "############### Creando modelo final con los mejores parámetros ################\n",
      "Accuracy de KNeighbors: 34.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Definimos el modelo a usar\n",
    "classifier = KNeighborsClassifier()\n",
    "model_name = 'KNeighbors'\n",
    "\n",
    "# Definimos los parámetros a explorar\n",
    "param_grid = {\n",
    "    \n",
    "    f'{model_name}__n_neighbors': [3, 5],\n",
    "    f'{model_name}__metric': ['euclidean', 'manhattan', 'cosine']\n",
    "}\n",
    "\n",
    "\n",
    "best_params = train_cv_models(model_name, classifier, param_grid, X_train, y_train)\n",
    "model_best_params = {k.split('__')[1]:v for k,v in best_params.items() if 'tfidf' not in k}\n",
    "best_classifier = KNeighborsClassifier(**model_best_params)\n",
    "model_stats = train_final_model(model_name, best_classifier, X_train, y_train)\n",
    "results.append(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Arboles de decisión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Primera búsqueda de parámetros ########################\n",
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   0.2s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   0.2s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=10; total time=   0.2s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=10; total time=   0.2s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=5; total time=   0.2s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=5; total time=   0.2s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=10; total time=   0.2s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=10; total time=   0.2s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   0.4s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   0.4s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=10; total time=   0.4s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=10; total time=   0.4s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=5; total time=   0.4s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=5; total time=   0.4s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=10; total time=   0.4s\n",
      "[CV] END DecisionTree__criterion=gini, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=10; total time=   0.5s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   0.8s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   0.7s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=10; total time=   0.6s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=10; total time=   0.8s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=5; total time=   0.6s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=5; total time=   0.8s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=10; total time=   0.6s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=5, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=10; total time=   0.7s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   1.2s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=10; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=10; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=5; total time=   0.9s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=5; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=10; total time=   0.9s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=3, DecisionTree__min_samples_split=10; total time=   1.0s\n",
      "Resultados primera búsqueda: {'DecisionTree__criterion': 'entropy', 'DecisionTree__max_depth': 10, 'DecisionTree__min_samples_leaf': 1, 'DecisionTree__min_samples_split': 5} \n",
      "\n",
      "######################## Segunda búsqueda de parámetros ######################## \n",
      "\n",
      "DEBUG: params after transform:  {'DecisionTree__criterion': ['entropy'], 'DecisionTree__max_depth': [9, 10, 11], 'DecisionTree__min_samples_leaf': [0, 1, 2], 'DecisionTree__min_samples_split': [4, 5, 6]}\n",
      "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=4; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=4; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=6; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=6; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=4; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=4; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   1.2s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=6; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=6; total time=   1.2s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=4; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=4; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=5; total time=   1.2s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=5; total time=   1.3s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=6; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=9, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=6; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=4; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=4; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=6; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=6; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=4; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=4; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=6; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=6; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=4; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=4; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=5; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=5; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=6; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=10, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=6; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=4; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=4; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=5; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=6; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=0, DecisionTree__min_samples_split=6; total time=   0.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=4; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=4; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=5; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=6; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=1, DecisionTree__min_samples_split=6; total time=   1.2s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=4; total time=   1.2s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=4; total time=   1.1s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=5; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=5; total time=   1.2s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=6; total time=   1.0s\n",
      "[CV] END DecisionTree__criterion=entropy, DecisionTree__max_depth=11, DecisionTree__min_samples_leaf=2, DecisionTree__min_samples_split=6; total time=   1.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "18 fits failed out of a total of 54.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\imblearn\\pipeline.py\", line 333, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **last_step_params[\"fit\"])\n",
      "  File \"c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_samples_leaf' parameter of DecisionTreeClassifier must be an int in the range [1, inf) or a float in the range (0.0, 1.0). Got 0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan 0.90065982 0.89809384 0.88929619\n",
      " 0.8797654  0.87646628 0.87463343        nan        nan        nan\n",
      " 0.93255132 0.92375367 0.91422287 0.90835777 0.90249267 0.896261\n",
      "        nan        nan        nan 0.93914956 0.93438416 0.9233871\n",
      " 0.9186217  0.91642229 0.90762463]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados segunda búsqueda: {'DecisionTree__criterion': 'entropy', 'DecisionTree__max_depth': 11, 'DecisionTree__min_samples_leaf': 1, 'DecisionTree__min_samples_split': 4} \n",
      "\n",
      "############### Creando modelo final con los mejores parámetros ################\n",
      "Accuracy de DecisionTree: 11.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Definimos el modelo a usar\n",
    "classifier = DecisionTreeClassifier()\n",
    "model_name = 'DecisionTree'\n",
    "\n",
    "# Definimos los parámetros a explorar\n",
    "param_grid = {\n",
    "    \n",
    "    f'{model_name}__criterion': ['gini', 'entropy'],\n",
    "    f'{model_name}__max_depth': [5, 10],\n",
    "    f'{model_name}__min_samples_split': [5, 10],\n",
    "    f'{model_name}__min_samples_leaf': [2, 3]\n",
    "}\n",
    "\n",
    "\n",
    "best_params = train_cv_models(model_name, classifier, param_grid, X_train, y_train)\n",
    "model_best_params = {k.split('__')[1]:v for k,v in best_params.items() if 'tfidf' not in k}\n",
    "\n",
    "best_classifier = DecisionTreeClassifier(**model_best_params)\n",
    "model_stats = train_final_model(model_name, best_classifier, X_train, y_train)\n",
    "results.append(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Primera búsqueda de parámetros ########################\n",
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV] END .......SVC__C=1, SVC__gamma=0.1, SVC__kernel=linear; total time=   6.5s\n",
      "[CV] END .......SVC__C=1, SVC__gamma=0.1, SVC__kernel=linear; total time=   5.3s\n",
      "[CV] END ..........SVC__C=1, SVC__gamma=0.1, SVC__kernel=rbf; total time=  12.3s\n",
      "[CV] END ..........SVC__C=1, SVC__gamma=0.1, SVC__kernel=rbf; total time=  13.2s\n",
      "[CV] END .........SVC__C=1, SVC__gamma=1, SVC__kernel=linear; total time=   5.9s\n",
      "[CV] END .........SVC__C=1, SVC__gamma=1, SVC__kernel=linear; total time=   6.7s\n",
      "[CV] END ............SVC__C=1, SVC__gamma=1, SVC__kernel=rbf; total time=   5.5s\n",
      "[CV] END ............SVC__C=1, SVC__gamma=1, SVC__kernel=rbf; total time=   8.4s\n",
      "[CV] END ......SVC__C=10, SVC__gamma=0.1, SVC__kernel=linear; total time=   3.5s\n",
      "[CV] END ......SVC__C=10, SVC__gamma=0.1, SVC__kernel=linear; total time=   3.4s\n",
      "[CV] END .........SVC__C=10, SVC__gamma=0.1, SVC__kernel=rbf; total time=   4.4s\n",
      "[CV] END .........SVC__C=10, SVC__gamma=0.1, SVC__kernel=rbf; total time=   3.4s\n",
      "[CV] END ........SVC__C=10, SVC__gamma=1, SVC__kernel=linear; total time=   4.8s\n",
      "[CV] END ........SVC__C=10, SVC__gamma=1, SVC__kernel=linear; total time=  10.0s\n",
      "[CV] END ...........SVC__C=10, SVC__gamma=1, SVC__kernel=rbf; total time=   5.7s\n",
      "[CV] END ...........SVC__C=10, SVC__gamma=1, SVC__kernel=rbf; total time=   4.7s\n",
      "Resultados primera búsqueda: {'SVC__C': 10, 'SVC__gamma': 1, 'SVC__kernel': 'rbf'} \n",
      "\n",
      "######################## Segunda búsqueda de parámetros ######################## \n",
      "\n",
      "DEBUG: params after transform:  {'SVC__C': [9.0, 10, 11.0], 'SVC__gamma': [0.9, 1, 1.1], 'SVC__kernel': ['rbf']}\n",
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n",
      "[CV] END ........SVC__C=9.0, SVC__gamma=0.9, SVC__kernel=rbf; total time=   5.2s\n",
      "[CV] END ........SVC__C=9.0, SVC__gamma=0.9, SVC__kernel=rbf; total time=   5.0s\n",
      "[CV] END ..........SVC__C=9.0, SVC__gamma=1, SVC__kernel=rbf; total time=   5.9s\n",
      "[CV] END ..........SVC__C=9.0, SVC__gamma=1, SVC__kernel=rbf; total time=   6.7s\n",
      "[CV] END ........SVC__C=9.0, SVC__gamma=1.1, SVC__kernel=rbf; total time=   3.8s\n",
      "[CV] END ........SVC__C=9.0, SVC__gamma=1.1, SVC__kernel=rbf; total time=   4.3s\n",
      "[CV] END .........SVC__C=10, SVC__gamma=0.9, SVC__kernel=rbf; total time=   3.1s\n",
      "[CV] END .........SVC__C=10, SVC__gamma=0.9, SVC__kernel=rbf; total time=   3.9s\n",
      "[CV] END ...........SVC__C=10, SVC__gamma=1, SVC__kernel=rbf; total time=   3.1s\n",
      "[CV] END ...........SVC__C=10, SVC__gamma=1, SVC__kernel=rbf; total time=   3.4s\n",
      "[CV] END .........SVC__C=10, SVC__gamma=1.1, SVC__kernel=rbf; total time=   3.3s\n",
      "[CV] END .........SVC__C=10, SVC__gamma=1.1, SVC__kernel=rbf; total time=   3.4s\n",
      "[CV] END .......SVC__C=11.0, SVC__gamma=0.9, SVC__kernel=rbf; total time=   2.8s\n",
      "[CV] END .......SVC__C=11.0, SVC__gamma=0.9, SVC__kernel=rbf; total time=   3.3s\n",
      "[CV] END .........SVC__C=11.0, SVC__gamma=1, SVC__kernel=rbf; total time=   3.9s\n",
      "[CV] END .........SVC__C=11.0, SVC__gamma=1, SVC__kernel=rbf; total time=   5.5s\n",
      "[CV] END .......SVC__C=11.0, SVC__gamma=1.1, SVC__kernel=rbf; total time=   5.3s\n",
      "[CV] END .......SVC__C=11.0, SVC__gamma=1.1, SVC__kernel=rbf; total time=   4.6s\n",
      "Resultados segunda búsqueda: {'SVC__C': 9.0, 'SVC__gamma': 1, 'SVC__kernel': 'rbf'} \n",
      "\n",
      "############### Creando modelo final con los mejores parámetros ################\n",
      "Accuracy de SVC: 29.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ccsar\\miniconda3\\envs\\env_nlp\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Definimos el modelo a usar\n",
    "classifier = SVC()\n",
    "model_name = 'SVC'\n",
    "\n",
    "# Definimos los parámetros a explorar\n",
    "param_grid = {\n",
    "    \n",
    "    f'{model_name}__C': [1, 10],\n",
    "    f'{model_name}__kernel': ['linear', 'rbf'],\n",
    "    f'{model_name}__gamma': [0.1, 1]\n",
    "}\n",
    "\n",
    "\n",
    "best_params = train_cv_models(model_name, classifier, param_grid, X_train, y_train)\n",
    "model_best_params = {k.split('__')[1]:v for k,v in best_params.items() if 'tfidf' not in k}\n",
    "\n",
    "best_classifier = SVC(**model_best_params)\n",
    "model_stats = train_final_model(model_name, best_classifier, X_train, y_train)\n",
    "results.append(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################## Primera búsqueda de parámetros ########################\n",
      "Fitting 2 folds for each of 16 candidates, totalling 32 fits\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=2, RandomForest__n_estimators=100; total time=   1.1s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=2, RandomForest__n_estimators=100; total time=   1.1s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=2, RandomForest__n_estimators=200; total time=   2.3s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=2, RandomForest__n_estimators=200; total time=   2.1s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=100; total time=   1.0s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=100; total time=   1.0s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   2.2s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   2.3s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=2, RandomForest__n_estimators=100; total time=   1.0s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=2, RandomForest__n_estimators=100; total time=   1.0s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=2, RandomForest__n_estimators=200; total time=   1.9s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=2, RandomForest__n_estimators=200; total time=   2.3s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=5, RandomForest__n_estimators=100; total time=   1.0s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=5, RandomForest__n_estimators=100; total time=   1.1s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   2.2s\n",
      "[CV] END RandomForest__max_depth=5, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   2.6s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=2, RandomForest__n_estimators=100; total time=   2.1s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=2, RandomForest__n_estimators=100; total time=   2.3s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=2, RandomForest__n_estimators=200; total time=   3.7s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=2, RandomForest__n_estimators=200; total time=   3.6s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=100; total time=   2.6s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=100; total time=   3.9s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   3.3s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   3.9s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=2, RandomForest__n_estimators=100; total time=   1.7s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=2, RandomForest__n_estimators=100; total time=   1.7s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=2, RandomForest__n_estimators=200; total time=   3.6s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=2, RandomForest__n_estimators=200; total time=   3.9s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=5, RandomForest__n_estimators=100; total time=   1.7s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=5, RandomForest__n_estimators=100; total time=   2.1s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   4.1s\n",
      "[CV] END RandomForest__max_depth=10, RandomForest__min_samples_leaf=4, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   4.7s\n",
      "Resultados primera búsqueda: {'RandomForest__max_depth': 10, 'RandomForest__min_samples_leaf': 2, 'RandomForest__min_samples_split': 5, 'RandomForest__n_estimators': 200} \n",
      "\n",
      "######################## Segunda búsqueda de parámetros ######################## \n",
      "\n",
      "DEBUG: params after transform:  {'RandomForest__max_depth': [9, 10, 11], 'RandomForest__min_samples_leaf': [1, 2, 3], 'RandomForest__min_samples_split': [4, 5, 6], 'RandomForest__n_estimators': [180.0, 200, 220.0]}\n",
      "Fitting 2 folds for each of 81 candidates, totalling 162 fits\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=4, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=4, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=4, RandomForest__n_estimators=200; total time=   3.2s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=4, RandomForest__n_estimators=200; total time=   3.3s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=4, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=4, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=5, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=5, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   4.2s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   3.5s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=5, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=5, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=6, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=6, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=6, RandomForest__n_estimators=200; total time=   3.5s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=6, RandomForest__n_estimators=200; total time=   3.3s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=6, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=1, RandomForest__min_samples_split=6, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=4, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=4, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=4, RandomForest__n_estimators=200; total time=   3.2s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=4, RandomForest__n_estimators=200; total time=   3.5s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=4, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=4, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   3.5s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=200; total time=   4.2s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=5, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=6, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=6, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=6, RandomForest__n_estimators=200; total time=   3.9s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=6, RandomForest__n_estimators=200; total time=   3.9s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=6, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=2, RandomForest__min_samples_split=6, RandomForest__n_estimators=220.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=3, RandomForest__min_samples_split=4, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=3, RandomForest__min_samples_split=4, RandomForest__n_estimators=180.0; total time=   0.0s\n",
      "[CV] END RandomForest__max_depth=9, RandomForest__min_samples_leaf=3, RandomForest__min_samples_split=4, RandomForest__n_estimators=200; total time=   3.7s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Definimos el modelo a usar\n",
    "classifier = RandomForestClassifier()\n",
    "model_name = 'RandomForest'\n",
    "\n",
    "# Definimos los parámetros a explorar\n",
    "param_grid = {\n",
    "    \n",
    "    f'{model_name}__n_estimators': [100, 200],\n",
    "    f'{model_name}__max_depth': [5, 10],\n",
    "    f'{model_name}__min_samples_split': [2, 5],\n",
    "    f'{model_name}__min_samples_leaf': [2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "best_params = train_cv_models(model_name, classifier, param_grid, X_train, y_train)\n",
    "model_best_params = {k.split('__')[1]:v for k,v in best_params.items() if 'tfidf' not in k}\n",
    "\n",
    "best_classifier = RandomForestClassifier(**model_best_params)\n",
    "model_stats = train_final_model(model_name, best_classifier, X_train, y_train)\n",
    "results.append(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 MLP Classifier (Multi-Layer Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Definimos el modelo a usar\n",
    "classifier = MLPClassifier()\n",
    "model_name = 'MLP'\n",
    "\n",
    "# Definimos los parámetros a explorar\n",
    "param_grid = {\n",
    "    \n",
    "    f'{model_name}__hidden_layer_sizes': [(100, 50), (50, 50)],\n",
    "    f'{model_name}__activation': ['relu', 'tanh'],\n",
    "    f'{model_name}__solver': ['adam', 'sgd'],\n",
    "    f'{model_name}__alpha': [0.001, 0.01],\n",
    "    f'{model_name}__learning_rate': ['constant', 'adaptive']\n",
    "}\n",
    "\n",
    "best_params = train_cv_models(model_name, classifier, param_grid, X_train, y_train)\n",
    "model_best_params = {k.split('__')[1]:v for k,v in best_params.items() if 'tfidf' not in k}\n",
    "\n",
    "best_classifier = MLPClassifier(**model_best_params)\n",
    "model_stats = train_final_model(model_name, best_classifier, X_train, y_train)\n",
    "results.append(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Definimos el modelo a usar\n",
    "classifier = xgb.XGBClassifier()\n",
    "model_name = 'XGB'\n",
    "\n",
    "# Definimos los parámetros a explorar\n",
    "param_grid = {\n",
    "    \n",
    "    f'{model_name}__max_depth': [3, 5],\n",
    "    f'{model_name}__learning_rate': [0.1, 0.3],\n",
    "    f'{model_name}__subsample': [0.8, 1.0],\n",
    "    f'{model_name}__colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "best_params = train_cv_models(model_name, classifier, param_grid, X_train, y_train)\n",
    "model_best_params = {k.split('__')[1]:v for k,v in best_params.items() if 'tfidf' not in k}\n",
    "\n",
    "best_classifier = xgb.XGBClassifier(**model_best_params)\n",
    "model_stats = train_final_model(model_name, best_classifier, X_train, y_train)\n",
    "results.append(model_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1_score'])\n",
    "res_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
